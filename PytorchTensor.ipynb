{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9491aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0ee57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# tensor,张量,是一个多维数组\n",
    "#推荐第四种创建方法,在内存管理时,会共享一大块内存,计算更快\n",
    "#1.用标准的数据格式来初始化tensor\n",
    "data = [[1, 2],[3, 4]]# python list\n",
    "x_data0 = torch.tensor(data)\n",
    "x_data1 = torch.tensor((1, 2, 3))  #tuple\n",
    "#x_data2 = torch.tensor({'a':5})  # RuntimeError: Could not infer dtype of dict\n",
    "print(x_data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75eac8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[[-1.1371, -0.3871,  0.5021, -2.9149, -1.6295],\n",
      "         [ 0.5967, -0.0721, -0.7843, -0.8640, -0.0751],\n",
      "         [ 0.0481, -0.0503,  0.2114, -0.9993, -0.2645],\n",
      "         [ 0.8848,  0.9769, -0.7354,  2.2181, -0.1744]],\n",
      "\n",
      "        [[-1.9941, -1.0963,  0.7392, -0.0870,  0.8535],\n",
      "         [-1.1744,  0.0852, -0.6836,  0.4369,  1.0487],\n",
      "         [-0.9459,  0.3791, -0.0123,  0.6430,  1.0226],\n",
      "         [-0.8113,  1.6142,  1.5126,  0.9987,  0.2323]],\n",
      "\n",
      "        [[-0.9552, -0.5129,  0.5832,  0.8393, -0.0778],\n",
      "         [ 0.6723,  0.9066,  1.0917,  0.7279,  1.7121],\n",
      "         [-0.7453,  0.1757, -1.6120, -0.5248,  0.4208],\n",
      "         [ 0.1334, -1.1884, -0.5246,  0.4482,  1.1182]]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n",
      "tensor([[1, 2, 1, 2, 1, 2, 4, 3, 0, 0],\n",
      "        [4, 3, 2, 0, 1, 4, 0, 3, 3, 1]])\n"
     ]
    }
   ],
   "source": [
    "#2.用torch的标准函数直接生成特殊的tensor\n",
    "data0 = torch.ones(1, 2, 3)  #全1矩阵\n",
    "data1 = torch.zeros(1, 3, 4)  # 全0矩阵\n",
    "data2 = torch.randn(3, 4, 5)  # 正态分布的随机数\n",
    "data3 = torch.eye(4, 5)  # 对角阵\n",
    "data4 = torch.randint(5,(2,10))  # 随机整数\n",
    "print(data0)\n",
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)\n",
    "print(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cbc3ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 4.4766, 3.0000])\n",
      "tensor([1., 2., 3.], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.仿照其他tensor生成\n",
    "data0 = torch.Tensor([1, 2, 3])\n",
    "data1 = torch.ones_like(data0) # shape参考data0,device也参考(把tensor放到GPU上)\n",
    "data2 = torch.empty_like(data0)\n",
    "print(data2)\n",
    "\n",
    "data0 = data0.cuda()\n",
    "print(data0)\n",
    "data1 = torch.ones_like(data0)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25a7c21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy data_ptr:  2628505150864\n",
      "torch data_ptr:  3694376586368\n",
      "tensor_from numpy data_ptr:  2628505150864\n",
      "ndarray_from_torch data_ptr:  2628505150864\n"
     ]
    }
   ],
   "source": [
    "#4.从numpy生成\n",
    "np_array = np.array([1, 2, 3])\n",
    "tensor_from_numpy1 = torch.from_numpy(np_array)\n",
    "#tensor_from_numpy2 = torch.Tensor(np_array)  # deepcopy\n",
    "np_array[0] = 100\n",
    "data_numpy = tensor_from_numpy1.numpy()\n",
    "\n",
    "# 下面是一个接口.可以查看具体的内存标识号\n",
    "def numpy_with_torch_tensor():\n",
    "  ndarray = np.array([1, 3, 4])\n",
    "  tensor = torch.tensor(ndarray)\n",
    "  tensor_from_numpy = torch.from_numpy(ndarray)\n",
    "  \n",
    "  print(\"numpy data_ptr: \", ndarray.ctypes.data)\n",
    "  print(\"torch data_ptr: \", tensor.data_ptr())\n",
    "  print(\"tensor_from numpy data_ptr: \", tensor_from_numpy.data_ptr())\n",
    "\n",
    "  ndarray_from_torch = tensor_from_numpy.numpy()\n",
    "  print(\"ndarray_from_torch data_ptr: \",   ndarray_from_torch.ctypes.data)\n",
    "\n",
    "numpy_with_torch_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02abbca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#二.数据类型转换\n",
    "tensor = torch.ones(4, 5)\n",
    "# tensor.type()\n",
    "def tensor_to_demo():\n",
    "    tensor = torch.ones(4, 5)\n",
    "    tensor0 = tensor.to(torch.int32)\n",
    "    tensor1 = tensor.to(tensor0)\n",
    "    # float16: 1bit(符号位) + 5bit(指数位) + 10bit(尾数) \n",
    "    # bfloat16：1bit(符号位) + 8bit(指数位) + 7bit(尾数)\n",
    "    # BF16背后的想法是通过降低数字的精度来减少计算能力和将张量相乘所需的能源消耗\n",
    "    tensor2 = tensor.to(torch.bfloat16) # 数据类型转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b808c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三.device转化\n",
    "def tensor_device_demo():\n",
    "  if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "  else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "  tensor = torch.randn(4, 5)\n",
    "  tensor_0 = tensor.to(device) # H2D   Host to device ,从主机内存复制数据到设备内存\n",
    "  tensor_1 = tensor.to('cpu')  #D2H      offload\n",
    "  tensor_2 = tensor.cuda()\n",
    "  tensor_3 = tensor.to(tensor_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f24fedd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "(1, 5)\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 四.怎么看两个tensor是不是占用用一个内存\n",
    "def reshape_demo():\n",
    "    data0 = torch.randn(4, 5)\n",
    "    data1 = data0.reshape(5, 4)\n",
    "    print(data0)\n",
    "    print(data1)\n",
    "#     print(data0.storage())  # 但是只能看表象\n",
    "#     print(data1.storage())\n",
    "    print(data0.data_ptr())\n",
    "    print(data1.data_ptr())  #  看指针,data0和data1是同一个tensor\n",
    "\n",
    "\n",
    "    \n",
    "# Pytorch 默认按行存储\n",
    "# stride,步长,是meta数据中重要的信息之一,按照元素级别来对应,numpy是按照Byte级别对应.理论来说,(2,3,4)的stride为(12,4,1)\n",
    "def reshape_view():\n",
    "    data0 = torch.randn(4, 5)\n",
    "    data1 = data0.view(5, 4)\n",
    "#     print(data0.stride())  # (5,1)\n",
    "#     print(data1.stride())  #(4,1)\n",
    "#     print(data0.data_ptr())  #3694376847104\n",
    "#     print(data1.data_ptr())  #3694376847104\n",
    "    \n",
    "\"\"\"\n",
    "那么,view和reshape肉眼看上去,效果一样,有什么区别?\n",
    "大部分情况下二者并无区别,只有针对不连续的数据才会发生区别,使用view会更加安全,永远不会重新搞一份数据出来\n",
    "当原来数据不连续时view会失败(比如,对data转秩之后,再view会失败),reshape当原来数据连续直接使用,反之重新copy一份数据,也就是说reshape永远可以运行成功.\n",
    "\"\"\"\n",
    "    \n",
    "def reshape_transpose():\n",
    "    data0 = torch.randn(4, 5)\n",
    "    data1 = data0.T  # (5,4),此处虽然做了转秩,但还是和data0共享同一份数据.Pytorch里做转秩数据不会真正的搬迁,但是stride会变化,stride对应做转秩\n",
    "    print(data0.stride())  # (5,1)\n",
    "    print(data1.stride())  # (1,5), 此时按理来说应该是(4,1)\n",
    "    print(data0.is_contiguous())  #True\n",
    "    print(data1.is_contiguous())  #False,不连续\n",
    "    \n",
    "    \n",
    "    data2 = data1.contiguous()\n",
    "    print(data2.is_contiguous())  #但是此时data2就不是和data1共享同一份数据量,发生了搬迁,相当于整块copy了一份\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "一个data的stride和shape是有一一对应的关系的,即shape(2,3,4)的stride为(12,4,1).\n",
    "若不满足这种关系,则发生了数据不连续(data.is_contiguous()),导致做计算式不可以成片的搬迁使用数据,导致效率降低\n",
    "此时需要手动使其连续(data.contiguous())\n",
    "那么在整个Pytorch算子中,只有transpose和premute操作会发生不连续的情况,因为往往在后面会手动使其连续\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "#     reshape_demo()\n",
    "#     reshape_view()\n",
    "    reshape_transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f240214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def permute_demo():\n",
    "    data0 = torch.randn(4, 5)  # 理论上stride=(5,1)\n",
    "    data1 = data0.permute(1, 0) #  shape=(5,4) ,理论上stride=(4,1)\n",
    "#     print(data0.stride())  \n",
    "#     print(data1.stride())  #(1,5)\n",
    "    print(data0.is_contiguous())\n",
    "    print(data1.is_contiguous())  # False\n",
    "\n",
    "    data2 = data1.contiguous()\n",
    "    print(data2.is_contiguous())\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    permute_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c525b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
