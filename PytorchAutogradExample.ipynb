{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81621f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1293, grad_fn=<VarBackward0>)\n",
      "tensor(0.1256, grad_fn=<VarBackward0>)\n",
      "tensor(0.1215, grad_fn=<VarBackward0>)\n",
      "tensor(0.1168, grad_fn=<VarBackward0>)\n",
      "tensor(0.1116, grad_fn=<VarBackward0>)\n",
      "tensor(0.1058, grad_fn=<VarBackward0>)\n",
      "tensor(0.0993, grad_fn=<VarBackward0>)\n",
      "tensor(0.0921, grad_fn=<VarBackward0>)\n",
      "tensor(0.0842, grad_fn=<VarBackward0>)\n",
      "tensor(0.0759, grad_fn=<VarBackward0>)\n",
      "tensor(0.0673, grad_fn=<VarBackward0>)\n",
      "tensor(0.0589, grad_fn=<VarBackward0>)\n",
      "tensor(0.0510, grad_fn=<VarBackward0>)\n",
      "tensor(0.0440, grad_fn=<VarBackward0>)\n",
      "tensor(0.0380, grad_fn=<VarBackward0>)\n",
      "tensor(0.0330, grad_fn=<VarBackward0>)\n",
      "tensor(0.0290, grad_fn=<VarBackward0>)\n",
      "tensor(0.0258, grad_fn=<VarBackward0>)\n",
      "tensor(0.0232, grad_fn=<VarBackward0>)\n",
      "tensor(0.0211, grad_fn=<VarBackward0>)\n",
      "tensor(0.0194, grad_fn=<VarBackward0>)\n",
      "tensor(0.0181, grad_fn=<VarBackward0>)\n",
      "tensor(0.0169, grad_fn=<VarBackward0>)\n",
      "tensor(0.0160, grad_fn=<VarBackward0>)\n",
      "tensor(0.0151, grad_fn=<VarBackward0>)\n",
      "tensor(0.0144, grad_fn=<VarBackward0>)\n",
      "tensor(0.0138, grad_fn=<VarBackward0>)\n",
      "tensor(0.0132, grad_fn=<VarBackward0>)\n",
      "tensor(0.0128, grad_fn=<VarBackward0>)\n",
      "tensor(0.0123, grad_fn=<VarBackward0>)\n",
      "tensor(0.0119, grad_fn=<VarBackward0>)\n",
      "tensor(0.0115, grad_fn=<VarBackward0>)\n",
      "tensor(0.0112, grad_fn=<VarBackward0>)\n",
      "tensor(0.0109, grad_fn=<VarBackward0>)\n",
      "tensor(0.0106, grad_fn=<VarBackward0>)\n",
      "tensor(0.0103, grad_fn=<VarBackward0>)\n",
      "tensor(0.0100, grad_fn=<VarBackward0>)\n",
      "tensor(0.0098, grad_fn=<VarBackward0>)\n",
      "tensor(0.0095, grad_fn=<VarBackward0>)\n",
      "tensor(0.0093, grad_fn=<VarBackward0>)\n",
      "tensor(0.0091, grad_fn=<VarBackward0>)\n",
      "tensor(0.0089, grad_fn=<VarBackward0>)\n",
      "tensor(0.0087, grad_fn=<VarBackward0>)\n",
      "tensor(0.0085, grad_fn=<VarBackward0>)\n",
      "tensor(0.0084, grad_fn=<VarBackward0>)\n",
      "tensor(0.0082, grad_fn=<VarBackward0>)\n",
      "tensor(0.0080, grad_fn=<VarBackward0>)\n",
      "tensor(0.0079, grad_fn=<VarBackward0>)\n",
      "tensor(0.0077, grad_fn=<VarBackward0>)\n",
      "tensor(0.0076, grad_fn=<VarBackward0>)\n",
      "tensor(0.0075, grad_fn=<VarBackward0>)\n",
      "tensor(0.0073, grad_fn=<VarBackward0>)\n",
      "tensor(0.0072, grad_fn=<VarBackward0>)\n",
      "tensor(0.0071, grad_fn=<VarBackward0>)\n",
      "tensor(0.0069, grad_fn=<VarBackward0>)\n",
      "tensor(0.0068, grad_fn=<VarBackward0>)\n",
      "tensor(0.0067, grad_fn=<VarBackward0>)\n",
      "tensor(0.0066, grad_fn=<VarBackward0>)\n",
      "tensor(0.0065, grad_fn=<VarBackward0>)\n",
      "tensor(0.0064, grad_fn=<VarBackward0>)\n",
      "tensor(0.0063, grad_fn=<VarBackward0>)\n",
      "tensor(0.0062, grad_fn=<VarBackward0>)\n",
      "tensor(0.0061, grad_fn=<VarBackward0>)\n",
      "tensor(0.0060, grad_fn=<VarBackward0>)\n",
      "tensor(0.0059, grad_fn=<VarBackward0>)\n",
      "tensor(0.0058, grad_fn=<VarBackward0>)\n",
      "tensor(0.0058, grad_fn=<VarBackward0>)\n",
      "tensor(0.0057, grad_fn=<VarBackward0>)\n",
      "tensor(0.0056, grad_fn=<VarBackward0>)\n",
      "tensor(0.0055, grad_fn=<VarBackward0>)\n",
      "tensor(0.0054, grad_fn=<VarBackward0>)\n",
      "tensor(0.0054, grad_fn=<VarBackward0>)\n",
      "tensor(0.0053, grad_fn=<VarBackward0>)\n",
      "tensor(0.0052, grad_fn=<VarBackward0>)\n",
      "tensor(0.0052, grad_fn=<VarBackward0>)\n",
      "tensor(0.0051, grad_fn=<VarBackward0>)\n",
      "tensor(0.0050, grad_fn=<VarBackward0>)\n",
      "tensor(0.0050, grad_fn=<VarBackward0>)\n",
      "tensor(0.0049, grad_fn=<VarBackward0>)\n",
      "tensor(0.0048, grad_fn=<VarBackward0>)\n",
      "tensor(0.0048, grad_fn=<VarBackward0>)\n",
      "tensor(0.0047, grad_fn=<VarBackward0>)\n",
      "tensor(0.0047, grad_fn=<VarBackward0>)\n",
      "tensor(0.0046, grad_fn=<VarBackward0>)\n",
      "tensor(0.0046, grad_fn=<VarBackward0>)\n",
      "tensor(0.0045, grad_fn=<VarBackward0>)\n",
      "tensor(0.0044, grad_fn=<VarBackward0>)\n",
      "tensor(0.0044, grad_fn=<VarBackward0>)\n",
      "tensor(0.0043, grad_fn=<VarBackward0>)\n",
      "tensor(0.0043, grad_fn=<VarBackward0>)\n",
      "tensor(0.0042, grad_fn=<VarBackward0>)\n",
      "tensor(0.0042, grad_fn=<VarBackward0>)\n",
      "tensor(0.0042, grad_fn=<VarBackward0>)\n",
      "tensor(0.0041, grad_fn=<VarBackward0>)\n",
      "tensor(0.0041, grad_fn=<VarBackward0>)\n",
      "tensor(0.0040, grad_fn=<VarBackward0>)\n",
      "tensor(0.0040, grad_fn=<VarBackward0>)\n",
      "tensor(0.0039, grad_fn=<VarBackward0>)\n",
      "tensor(0.0039, grad_fn=<VarBackward0>)\n",
      "tensor(0.0039, grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def autograd_demo_v1():\n",
    "    torch.manual_seed(0)\n",
    "    x = torch.ones(5, requires_grad=True)\n",
    "    w = torch.randn(5, 5, requires_grad=True)\n",
    "    b = torch.randn_like(x)\n",
    "    lable = torch.Tensor([0, 0, 1, 0, 0])  # 五分类\n",
    "    \n",
    "    for i in range(100):\n",
    "        if w.grad is not None:\n",
    "            w.grad.zero_ro_()\n",
    "            \n",
    "        z = torch.matmul(w, x) + b  #  linear layer\n",
    "        output = torch.sigmoid(z)\n",
    "#         output = register_hook(hook)\n",
    "        output.retain_grad()\n",
    "        loss = (output-lable).var()\n",
    "        loss.backward()\n",
    "#         print(w.grad)\n",
    "        w = w - 0.8 * w.grad    # 更新权重,在第一轮循环的时候w还是叶子节点,第二轮循环时,就变成一个输出,此时就是activation,反向传播结束时,会被丢掉\n",
    "        w.retain_grad()  # 保留中间值的梯度,但这种方法不是最好的,因为在底层已经修改了w的属性is leaf?\n",
    "        print(loss)\n",
    "        \n",
    "\n",
    "        \n",
    "def autograd_demo_v2():\n",
    "    torch.manual_seed(0)\n",
    "    x = torch.ones(5, requires_grad=True)\n",
    "    w = torch.randn(5, 5, requires_grad=True)\n",
    "    b = torch.randn_like(x)\n",
    "    lable = torch.Tensor([0, 0, 1, 0, 0])  # 五分类\n",
    "    \n",
    "    for i in range(100):\n",
    "#         if w.grad is not None:\n",
    "#             w.grad.zero_ro_()\n",
    "        z = torch.matmul(w, x) + b  #  linear layer\n",
    "        output = torch.sigmoid(z)\n",
    "#         output = register_hook(hook)\n",
    "        output.retain_grad()\n",
    "        loss = (output-lable).var()\n",
    "        loss.backward()\n",
    "#         w = w - 0.8 * w.grad    # 更新权重,在第一轮循环的时候w还是叶子节点,第二轮循环时,就变成一个输出,此时就是activation,反向传播结束时,会被丢掉\n",
    "#         w.retain_grad()  # 保留中间值的梯度,但这种方法不是最好的,因为在底层已经修改了w的属性is leaf?\n",
    "        w.data.sub_(0.8 * w.grad)  # 这种方式是比较科学的,修改data属性(raw data)\n",
    "        w.grad = None\n",
    "        print(loss)\n",
    "        \n",
    "def autograd_demo_v3():\n",
    "    torch.manual_seed(0)\n",
    "    x = torch.ones(5, requires_grad=True)\n",
    "    y = torch.randn(5, 5, requires_grad=True) # 叶子节点\n",
    "    b = torch.randn_like(x)\n",
    "\n",
    "    for i in range(100):\n",
    "        if i > 0:\n",
    "            y.grad.zero_()\n",
    "        z = torch.matmul(y, x) + b # linear layer    \n",
    "        output = torch.sigmoid(z)\n",
    "        label = torch.Tensor([0, 0, 1, 0, 0])\n",
    "        loss = (output-label).var() # l2 loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # tensor a : requires_grad,  --> a.sub_(b): 对它的数据进行了更新；\n",
    "        # pytorch check： 对我们需要更新梯度的tensor 禁止用 replace操作；\n",
    "        # torch.no_grad(): 忽略这些警告，运行 replace 操作；\n",
    "        with torch.no_grad(): # replace   # 适用于需要批量绕过\n",
    "          y.sub_(0.5 * y.grad)\n",
    "    \n",
    "        print(\"loss: \", loss)\n",
    "if __name__ == '__main__':\n",
    "#     autograd_demo_v1()\n",
    "    autograd_demo_v2()\n",
    "#     autograd_demo_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a861f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "自动求导机制在下面网址有一个阶段性要点总结\n",
    "https://github.com/Elvin-Ma/deep_learning_theory/tree/main/21-pytorch-autograd\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
